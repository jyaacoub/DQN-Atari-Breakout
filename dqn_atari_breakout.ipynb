{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GiannisMitr/DQN-Atari-Breakout/blob/master/dqn_atari_breakout.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAfMgPNBvziQ"
      },
      "source": [
        "### Processing game image \n",
        "\n",
        "Raw atari images are large, 210x160x3 by default. However, we don't need that level of detail in order to learn them.\n",
        "\n",
        "We can thus save a lot of time by preprocessing game image, including\n",
        "* Resizing to a smaller shape, 64 x 64\n",
        "* Converting to grayscale\n",
        "* Cropping irrelevant image parts (top & bottom)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\jyaac\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ale_py\\roms\\utils.py:90: DeprecationWarning: SelectableGroups dict interface is deprecated. Use select.\n",
            "  for external in metadata.entry_points().get(self.group, []):\n"
          ]
        }
      ],
      "source": [
        "# tf imports\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "# Gym imports\n",
        "from gym.core import ObservationWrapper\n",
        "from gym.core import Wrapper\n",
        "from gym.spaces.box import Box\n",
        "import gym\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ENVIRONMENT AND PROCESSING:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BzuRB5OEvziV"
      },
      "outputs": [],
      "source": [
        "class PreprocessAtari(ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        \"\"\"A gym wrapper that crops, scales image into the desired shapes and optionally grayscales it.\"\"\"\n",
        "        ObservationWrapper.__init__(self,env)\n",
        "        \n",
        "        self.img_size = (84, 84)\n",
        "        self.observation_space = Box(0.0, 1.0, (self.img_size[0], self.img_size[1], 1))\n",
        "\n",
        "    def observation(self, img):\n",
        "        \"\"\"what happens to each observation\"\"\"\n",
        "        \n",
        "        # crop image (top and bottom, top from 34, bottom remove last 16)\n",
        "        img = img[34:-16, :, :]\n",
        "        \n",
        "        # resize image\n",
        "        img = cv2.resize(img, self.img_size)\n",
        "        \n",
        "        img = img.mean(-1,keepdims=True)\n",
        "        \n",
        "        img = img.astype('float32') / 255.\n",
        "        return img\n",
        "\n",
        "class FrameBuffer(Wrapper):\n",
        "    def __init__(self, env, n_frames=4, dim_order='tensorflow'):\n",
        "        \"\"\"A gym wrapper that reshapes, crops and scales image into the desired shapes\"\"\"\n",
        "        super(FrameBuffer, self).__init__(env)\n",
        "        self.dim_order = dim_order\n",
        "\n",
        "        height, width, n_channels = env.observation_space.shape\n",
        "        obs_shape = [height, width, n_channels * n_frames]\n",
        "\n",
        "        self.observation_space = Box(0.0, 1.0, obs_shape)\n",
        "        self.framebuffer = np.zeros(obs_shape, 'float32')\n",
        "        \n",
        "    def reset(self):\n",
        "        \"\"\"resets breakout, returns initial frames\"\"\"\n",
        "        self.framebuffer = np.zeros_like(self.framebuffer)\n",
        "        self.update_buffer(self.env.reset())\n",
        "        return self.framebuffer\n",
        "    \n",
        "    def step(self, action):\n",
        "        \"\"\"plays breakout for 1 step, returns frame buffer\"\"\"\n",
        "        new_img, reward, done, info = self.env.step(action)\n",
        "        self.update_buffer(new_img)\n",
        "        return self.framebuffer, reward, done, info\n",
        "    \n",
        "    def update_buffer(self, img):\n",
        "        offset = self.env.observation_space.shape[-1]\n",
        "        axis = -1\n",
        "        cropped_framebuffer = self.framebuffer[:,:,:-offset]\n",
        "        \n",
        "        self.framebuffer = np.concatenate([img, cropped_framebuffer], axis=axis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fVuXynjSvzie",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "def make_env():\n",
        "    env = gym.make(\"BreakoutDeterministic-v4\")\n",
        "    env = PreprocessAtari(env)\n",
        "    env = FrameBuffer(env, n_frames=4, dim_order='tensorflow')\n",
        "    return env\n",
        "\n",
        "#Instatntiate gym Atari-Breakout environment\n",
        "env = make_env()\n",
        "env.reset()\n",
        "n_actions = env.action_space.n\n",
        "state_dim = env.observation_space.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## NETWORK AND EVALUTATING:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DeepQNetwork(object):\n",
        "    def __init__(self, n_actions, epsilon=0):\n",
        "        \"\"\"\n",
        "        An implementation of the exact network used in the Atari paper. So not many arguments needed.\n",
        "\n",
        "        Args:\n",
        "            num_actions (int): The number of possible actions. This will define the output shape of the model.\n",
        "            learning_rate (float, optional): Defaults to 0.1.\n",
        "            batch_size (int, optional): Defaults to the size that is used in the paper.\n",
        "        \"\"\"\n",
        "        self.network = self.__build_model(n_actions)\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    @staticmethod\n",
        "    def __build_model(num_actions:int) -> tf.keras.Model:\n",
        "        \"\"\"\n",
        "        This function builds the exact DQN model from the Atari paper.\n",
        "            Input shape is (84, 84, 4).\n",
        "                - This is the preprocessed images of the last 4 frames in the history\n",
        "\n",
        "            1st Hidden layer convolves 16 8x8 filters with stride 4.\n",
        "                - followed by rectifier nonlinear\n",
        "\n",
        "            2nd hidden layer convolves 32 4x4 filters with stride 2.\n",
        "                - again followed by rectifier nonlinearity\n",
        "\n",
        "            Output layer is a fully connected linear layer.\n",
        "                - shape -> (a, ) where a is the number of actions\n",
        "                - the ouput corresponds to the predicted Q-values\n",
        "\n",
        "        Args:\n",
        "            num_actions (int): this determines the output shape\n",
        "\n",
        "        Returns:\n",
        "            tf.keras.model: The DQN model from the paper\n",
        "        \"\"\"\n",
        "        # first layer takes in the 4 grayscale cropped image\n",
        "        input_lyr = tf.keras.layers.Input((84,84,4), name=\"Input_last_4_frames\")\n",
        "        \n",
        "        # convolutional layers \n",
        "        x = tf.keras.layers.Conv2D(32, (8,8), activation='relu', strides=4, use_bias=False, input_shape=(84,84,4), name=\"Hidden_layer_1\")(input_lyr)\n",
        "        x = tf.keras.layers.Conv2D(64, (4,4), activation='relu', strides=2, use_bias=False, name=\"Hidden_layer_2\")(x)\n",
        "        x = tf.keras.layers.Conv2D(64, (3,3), activation='relu', strides=1, use_bias=False, name=\"Hidden_layer_3\")(x)\n",
        "        x = tf.keras.layers.Conv2D(1024, (7,7), activation='relu', strides=1, use_bias=False, name=\"Hidden_layer_4\")(x)\n",
        "\n",
        "        # flattening for dense output\n",
        "        x = tf.keras.layers.Flatten(name=\"Final_flatten\")(x)\n",
        "        x = tf.keras.layers.Dense(num_actions, activation='linear')(x)\n",
        "\n",
        "        return tf.keras.Model(inputs=input_lyr, outputs=x, name=\"ATARI_DQN\")\n",
        "\n",
        "    def get_qvalues(self, state_t):\n",
        "        return self.network.predict(np.asarray(state_t))\n",
        "    \n",
        "    def sample_actions(self, qvalues):\n",
        "        epsilon = self.epsilon\n",
        "        batch_size, n_actions = qvalues.shape\n",
        "        random_actions = np.random.choice(n_actions, size=batch_size)\n",
        "        best_actions = qvalues.argmax(axis=-1)\n",
        "        should_explore = np.random.choice([0, 1], batch_size, p = [1-epsilon, epsilon])\n",
        "        return np.where(should_explore, random_actions, best_actions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "b0oplbmws0aQ"
      },
      "outputs": [],
      "source": [
        "#Evaluate agents performance, in a number of games\n",
        "def evaluate(env, agent, n_games=1, greedy=False, t_max=10000):\n",
        "    \"\"\" Plays n_games full games. If greedy, picks actions as argmax(qvalues). Returns mean reward. \"\"\"\n",
        "    rewards = []\n",
        "    s = env.reset()\n",
        "    for _ in range(n_games):\n",
        "        reward = 0\n",
        "        for _ in range(t_max):\n",
        "            qvalues = agent.get_qvalues([s])\n",
        "            action = qvalues.argmax(axis=-1)[0] if greedy else agent.sample_actions(qvalues)[0]\n",
        "            s, r, done, _ = env.step(action)\n",
        "         \n",
        "            reward += r\n",
        "            if done: \n",
        "              s = env.reset()\n",
        "              break\n",
        "        rewards.append(reward)\n",
        "    return np.mean(rewards)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From C:\\Users\\jyaac\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "358.0\n"
          ]
        }
      ],
      "source": [
        "agent = DeepQNetwork(n_actions, epsilon=0.5)\n",
        "agent.network.load_weights('dqn_model_og.h5')\n",
        "agent.epsilon = 0.001\n",
        "print(evaluate(make_env(), agent, n_games=1))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "machine_shape": "hm",
      "name": "dqn_atari_breakout.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
